# OLLM
OLLM is a tool for detecting functional bugs.


## Artifact Description
This is the artifact for the paper: Detecting Non-Crash Functional Bugs in Android Apps Using Multimodal Large Language Models.

The artifact shows:

1) Source code for OLLM (./code/).
2) Experiment results generated by OLLM presented in the paper(./Experiments/Prompt Feedback/).
3) Programs (i.e., APKs), bug reports, and text sequence generated in the paper (./Experiments/Bug Reproduction and Sequence Generation/).
4) Prompts used for OLLM(./Prompts/).
5) Documentation showing how OLLM can be used.


## Run OLLM
1) Create Android Emulator Pixel 2, API 28， Android 9.0 ('Pie') x86 in Android Studio, and open the emulator.
2) Get the emulator device number, in Linux cmd run ’adb devices’, and replace the number in d = Device("emulator-5554") in the source code with the device number of your own.
3) Install the APK and drag the APK into the emulator. Open the app in your emulator.
4) In the same category with the source code file, create a txt file named action.txt, and store the action text in it. the format of the action text should be 'Action NameOfUIElement'.
   For Example:
```sh
click Allow
click Allow
click com.ichi2.anki:id/action_sync
click LOG IN
click Email address
```
The action text can either be defined by the human tester or by an automatic sequence generator.
5) Change the folder name in the script for each bug, the variable folder_name = 'XXX', XXX is the id of the bug.
6) Get the source code from the code category(./code/) and put it in the same category of the action.txt


## Description of Input Files of OLLM:
* The input file of the tool is a Txt file named 'action.txt'.
* The APK file of the app.


## Description of Result Files Generated By OLLM:
* The output path of the tool is in ``/*bugid/``.
* Open the folder and you'll see the .xml file and screenshot of each step of the app after execution.
* The text sequence containing text extracted from UI and the actions will be stored in a file named ``prompt_v1_*bugid.txt``.
* Use the pre-prompt in the prompt folder (./prompt/) and the text sequence for LLMs the format should be pre-prompt, LLM Response, text sequence, LLM decision.

## Description Experiment Files:
* For each research question, there is a folder containing the experiment data as evidence of the conclusion. For example, (./Experiments/RQ1 Effectiveness and Efficiency of detecting NCF bugs)
* Each research question contains an Excel form that records the bug id, the bug name, the result of the experiment, and the URL of the LLM response.
* There's a test sequence folder containing all of the test sequences we generated for the bugs.
* For each experiment, there's a folder containing the prompts (for example, ./Experiments/RQ1 Effectiveness and Efficiency of detecting NCF bugs/Prompt of TP/Prompts) and a folder containing the LLM responses(./Experiments/RQ1 Effectiveness and Efficiency of detecting NCF bugs/Prompt of TP/LLM response).

## Text Sequence Generation Tool
* The text sequence generation tool used in the paper can automatically browse the app and generate text sequences in the desired format (action+ GUI text extraction).
* Download the tool: https://drive.google.com/file/d/1_rtSx0uVhh9QeOkyAXSCrPqqpPmDxh_5/view?usp=sharing

## How to Run Text Sequence Generation Tool:
* Extract the zip file
* Put the APK file in dataset/unfinished folder
* In the folder, run 
```sh
python2 ./RunTest.py
```
