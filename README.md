# OLLM
OLLM is a tool for detecting functional bugs.


## Artifact Description
This is the artifact for the paper: Detecting Non-Crash Functional Bugs in Android Apps Using Multimodal Large Language Models.

The artifact shows:

1) Source code for OLLM (./code/).
2) Experiment results generated by OLLM presented in the paper(./Experiments/Prompt Feedback/).
3) Programs (i.e., APKs), bug reports, and text sequence generated in the paper (./Experiments/Bug Reproduction and Sequence Generation/).
4) Prompts used for OLLM(./Prompts/).
5) Documentation showing how OLLM can be used.


## Run OLLM
1) Create Android Emulator Pixel 2, API 28， Android 9.0 ('Pie') x86 in Android Studio, and open the emulator.
2) Get the emulator device number, in Linux cmd run ’adb devices’, and replace the number in d = Device("emulator-5554") in the source code with the device number of your own.
3) Install the APK and drag the APK into the emulator. Open the app in your emulator.
4) In the same category with the source code file, create a txt file named action.txt, and store the action text in it. the format of the action text should be 'Action NameOfUIElement'.
   For Example:
```sh
click Allow
click Allow
click com.ichi2.anki:id/action_sync
click LOG IN
click Email address
```
The action text can either be defined by the human tester or by an automatic sequence generator.
5) Change the folder name in the script for each bug, the variable folder_name = 'XXX', XXX is the id of the bug.
6) Get the source code from the code category(./code/) and put it in the same category of the action.txt


##Description of Input Files of OLLM:
* The input file of the tool is a Txt file named 'action.txt'.
* The APK file of the app.


## Description of Result Files Generated By OLLM:
* The output path of the tool is in ``/*bugid/``.
* Open the folder and you'll see the .xml file and screenshot of each step of the app after execution.
* The text sequence containing text extracted from UI and the actions will be stored in a file named ``prompt_v1_*bugid.txt``.
* Use the pre-prompt in the prompt folder (./prompt/) and the text sequence for LLMs the format should be pre prompt, LLM Response, text sequence, LLM decision.

